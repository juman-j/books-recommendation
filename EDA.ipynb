{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv(\"data/raw_data/Books.csv\", dtype={3: \"str\"})\n",
    "ratings_df = pd.read_csv(\"data/raw_data/Ratings.csv\")\n",
    "users_df = pd.read_csv(\"data/raw_data/Users.csv\")\n",
    "\n",
    "books_df = books_df[:50000]\n",
    "ratings_df = ratings_df[:50000]\n",
    "users_df = users_df[:50000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 0\n",
      "Number of unique books: 50000\n"
     ]
    }
   ],
   "source": [
    "duplicats = books_df.duplicated().sum()\n",
    "books_nunique = books_df[\"ISBN\"].nunique()\n",
    "\n",
    "print(f\"Number of duplicates: {duplicats}\")\n",
    "print(f\"Number of unique books: {books_nunique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books_df.shape before = (50000, 5)\n",
      "books_df.shape after = (50000, 5)\n"
     ]
    }
   ],
   "source": [
    "books_df.drop([\"Image-URL-S\", \"Image-URL-M\", \"Image-URL-L\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "books_df.rename(\n",
    "    columns={\n",
    "        \"ISBN\": \"isbn\",\n",
    "        \"Book-Title\": \"book_title\",\n",
    "        \"Book-Author\": \"book_author\",\n",
    "        \"Year-Of-Publication\": \"year\",\n",
    "        \"Publisher\": \"publisher\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"books_df.shape before = {books_df.shape}\")\n",
    "\n",
    "books_df[books_df.columns[3]] = pd.to_numeric(\n",
    "    books_df[books_df.columns[3]], errors=\"coerce\"\n",
    ")  # If ‘coerce’, then invalid parsing will be set as NaN.\n",
    "books_df.dropna(subset=[\"year\"], axis=0, inplace=True)\n",
    "books_df[\"year\"] = books_df[\"year\"].astype(int)\n",
    "\n",
    "print(f\"books_df.shape after = {books_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>50000</td>\n",
       "      <td>46241</td>\n",
       "      <td>23359</td>\n",
       "      <td>4994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Wuthering Heights</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>Harlequin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>192</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn         book_title   book_author  publisher\n",
       "count        50000              50000         50000      50000\n",
       "unique       50000              46241         23359       4994\n",
       "top     0195153448  Wuthering Heights  Stephen King  Harlequin\n",
       "freq             1                  9           192       1206"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.describe(include=\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1959.898840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>260.465229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1996.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2030.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               year\n",
       "count  50000.000000\n",
       "mean    1959.898840\n",
       "std      260.465229\n",
       "min        0.000000\n",
       "25%     1990.000000\n",
       "50%     1996.000000\n",
       "75%     2000.000000\n",
       "max     2030.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.describe(include=np.number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books with incorrect year of publication = 874\n",
      "This represents 2 percent of the total number of books.\n"
     ]
    }
   ],
   "source": [
    "filtered_df = books_df[\n",
    "    (books_df[\"year\"] == 0) | (books_df[\"year\"] > 2004)\n",
    "]  # The dataset was collected in 2004\n",
    "filtered_df = filtered_df.astype({\"year\": float})\n",
    "filtered_df.loc[:, \"year\"] = np.nan  # Replacing the numbers with the missing ones.\n",
    "\n",
    "\n",
    "print(f\"Number of books with incorrect year of publication = {len(filtered_df)}\")\n",
    "print(f\"This represents {round(len(filtered_df)/len(books_df)*100)} percent of the total number of books.\")\n",
    "\n",
    "books_df.drop(index=filtered_df.index, inplace=True) # drop this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the missing values after splitting the dataset into training and test dataset to prevent data leakage.\n",
    "But due to the fact that this is all 2 percent of the sample, as part of the test assignment, I will simply delete these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.rename(\n",
    "    columns={\n",
    "        \"ISBN\": \"isbn\",\n",
    "        \"User-ID\": \"user_id\",\n",
    "        \"Book-Rating\": \"rating\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   50000.00\n",
       "mean        3.18\n",
       "std         3.93\n",
       "min         0.00\n",
       "25%         0.00\n",
       "50%         0.00\n",
       "75%         7.00\n",
       "max        10.00\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "ratings_df['rating'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.rename(\n",
    "    columns={\"User-ID\": \"user_id\"},\n",
    "    inplace=True,\n",
    ")\n",
    "users_df.drop([\"Location\", \"Age\"], axis=1, inplace=True)\n",
    "users_df = users_df.sample(frac=0.3) # crop dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 0\n",
      "Number of unique users: 15000\n"
     ]
    }
   ],
   "source": [
    "duplicats = users_df.duplicated().sum()\n",
    "user_nunique = users_df[\"user_id\"].nunique()\n",
    "\n",
    "print(f\"Number of duplicates: {duplicats}\")\n",
    "print(f\"Number of unique users: {user_nunique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_users = users_df.merge(right=ratings_df, how=\"inner\", on=\"user_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only the rows that are in both datasets.\n",
    "ratings_users = ratings_users.merge(right=books_df, how=\"inner\", on=\"isbn\")\n",
    "ratings_users.drop([\"book_title\", \"book_author\", \"year\", \"publisher\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6575</td>\n",
       "      <td>0001714600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6575</td>\n",
       "      <td>0028604199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6575</td>\n",
       "      <td>0028606736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6575</td>\n",
       "      <td>0030640075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6575</td>\n",
       "      <td>0060002093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  rating\n",
       "0     6575  0001714600       0\n",
       "1     6575  0028604199       0\n",
       "2     6575  0028606736       0\n",
       "3     6575  0030640075       0\n",
       "4     6575  0060002093       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_users.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __drop_zero_rating(data: pd.DataFrame, groupby_column: str) -> pd.DataFrame:\n",
    "    grouped_sum_ratings = data.groupby([groupby_column]).sum()[\"rating\"]\n",
    "    zero_ratings = grouped_sum_ratings[grouped_sum_ratings == 0]\n",
    "    data = data[~data[groupby_column].isin(zero_ratings.index)]  # drop zero_ratings\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def __more_than_ratings(data: pd.DataFrame, groupby_column: str, min_ratings: int) -> pd.DataFrame:\n",
    "    grouped_count_ratings = data.groupby([groupby_column]).count()[\"rating\"]\n",
    "    more_than = grouped_count_ratings[grouped_count_ratings >= min_ratings]\n",
    "    data = data[data[groupby_column].isin(more_than.index)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_up_data(\n",
    "    data: pd.DataFrame, groupby_column: str, min_ratings: int\n",
    ") -> pd.DataFrame:\n",
    "    data = __drop_zero_rating(data, groupby_column)\n",
    "    data = __more_than_ratings(data, groupby_column, min_ratings)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_users = clean_up_data(data=ratings_users, groupby_column=\"user_id\", min_ratings=10)\n",
    "ratings_users = clean_up_data(data=ratings_users, groupby_column=\"isbn\", min_ratings=5)\n",
    "ratings_users.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the test task, I will **remove** books that had **less than 5 ratings**. 44.7% of the books were graded only once. I don't want to keep these books for training the model, because the rating matrix will be extremely sparse, which will have a bad effect on the quality of the predictions. \n",
    "\n",
    "It is potentially possible to cluster books with a small number of ratings and use this clustering for users with specific preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    data: pd.DataFrame, test_size: float\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = pd.DataFrame(columns=data.columns)\n",
    "    test_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    for _, user_data in data.groupby(\"user_id\"):\n",
    "        rated_data = user_data[user_data[\"rating\"] > 0]\n",
    "        test_count = int(len(rated_data) * test_size)\n",
    "\n",
    "        test_indices = np.random.choice(\n",
    "            rated_data.index, size=test_count, replace=False\n",
    "        )\n",
    "        user_test_data = user_data.loc[test_indices]\n",
    "        user_train_data = user_data.drop(test_indices)\n",
    "\n",
    "        test_data = pd.concat([test_data, user_test_data])\n",
    "        train_data = pd.concat([train_data, user_train_data])\n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = train_test(ratings_users, test_size=0.2)\n",
    "# crop_train_data = train_data.iloc[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 233 zeros in the trimmed sample.\n",
      "There are 154 non-zeros in the trimmed sample.\n"
     ]
    }
   ],
   "source": [
    "null_vals = train_data[train_data[\"rating\"] == 0].shape[0]\n",
    "not_null_vals = train_data[train_data[\"rating\"] != 0].shape[0]\n",
    "\n",
    "print(f\"There are {null_vals} zeros in the trimmed sample.\")\n",
    "print(f\"There are {not_null_vals} non-zeros in the trimmed sample.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A: pd.DataFrame = train_data.pivot(\n",
    "    values=\"rating\", index=\"user_id\", columns=\"isbn\"\n",
    ").infer_objects(copy=False)\n",
    "A.fillna(0, inplace=True)\n",
    "A = A.astype(int)\n",
    "A_norm: pd.DataFrame = (A - np.min(A)) / (np.max(A) - np.min(A))\n",
    "test_data[\"rating\"] = (test_data[\"rating\"] - np.min(A)) / (np.max(A) - np.min(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD(A: np.ndarray, d: int, learning_rate: float, lambda_reg, n_iters):\n",
    "    mu = A.sum()/(A!=0).sum()\n",
    "    non_zero = (A!=0).sum()\n",
    "\n",
    "    # Initialize matrices U and V with dimensions (rows of A, d) and (d, columns of A), filled with the mean value mu\n",
    "    U = np.zeros((A.shape[0], d)) + mu\n",
    "    V = np.zeros((d, A.shape[1])) + mu\n",
    "\n",
    "    mse_start = 0\n",
    "    index, zero_index, mse = [], [], []\n",
    "\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            if A[i][j]>0:\n",
    "                index.append([i, j])\n",
    "                mse_start += ((A[i, j] - np.dot(U[i,:], V[:,j])) ** 2) / non_zero\n",
    "            else:\n",
    "                zero_index.append([i, j])\n",
    "\n",
    "    # Stochastic Gradient Descent loop over specified number of iterations\n",
    "    for n in range(n_iters):\n",
    "        choice = np.random.randint(0, len(index))\n",
    "        ij = index[choice]\n",
    "        i = ij[0]\n",
    "        j = ij[1]\n",
    "\n",
    "        # Update factors U and V for chosen element (i, j)\n",
    "        for k in range(d):\n",
    "            U[i, k] = U[i, k] + learning_rate * ((A[i][j] - np.dot(U[i, :], V[:, j])) * V[k, j] - lambda_reg * U[i, k])\n",
    "            V[k, j] = V[k, j] + learning_rate * ((A[i][j] - np.dot(U[i, :], V[:, j])) * U[i, k] - lambda_reg * V[k, j])\n",
    "\n",
    "        current_mse = 0\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                if A[i,j]>0:\n",
    "                    current_mse += ((A[i, j] - np.dot(U[i,:], V[:,j])) ** 2) / non_zero\n",
    "        mse.append(current_mse)\n",
    "\n",
    "    return U, V, mse_start, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, V, mse_start, mse = SVD(A_norm.values, 10, learning_rate=0.005, lambda_reg=0.02, n_iters=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(A: pd.DataFrame, test_data: pd.DataFrame, U: np.ndarray, V: np.ndarray):\n",
    "    U_series = pd.Series(data=list(U), index=A.index)\n",
    "    V_series = pd.Series(data=list(V.T), index=A.columns)\n",
    "    len_test_sample = len(test_data)\n",
    "    mse_test = 0\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        true_val = row.iloc[2]\n",
    "        prediction = np.dot(U_series[row.iloc[0]], V_series[row.iloc[1]])\n",
    "        if prediction > 1:\n",
    "            prediction = 1\n",
    "\n",
    "        mse_test += ((true_val - prediction) ** 2) / len_test_sample\n",
    "\n",
    "    return mse_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test sample: 0.06301236579139169\n"
     ]
    }
   ],
   "source": [
    "mse_test = evaluate(A_norm, test_data, U, V)\n",
    "print(f\"MSE on the test sample: {mse_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "books_recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
